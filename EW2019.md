# Is running containers on an embedded device a good idea? #
In the real world, intermodal containers have completely changed logistics and transportation by providing a standardized format for a metal box that could store many different kinds of goods.  
Ships, docks, cranes, trucks, forklifts have been designed around that format and that made the shipping industry much more efficient.  
Embracing the intermodal container took a few decades, and still today not all the shipments are done using containers, because for some kind of good that would simply not make sense, but this standard format has reduced costs, eliminated complexity in the infrastructure and made the whole industry much more efficient than it was just 50 years ago.  
Containers are now also a popular concept in the software world, and the analogy with the "real" world seems to work.  
Software containers provide a standard way to package an application or a service and run it inside a "sandboxed" environment (more about this later), and this allowed the development of standard tools to create, distribute, run and monitor containers. The main difference is that it didn't take decades for containers to become widely used in the software industry.  
This started on servers and cloud-based systems, where deploying multiple instances of the same software component to handle changing workloads and move them between development, testing and production environments is critical. Many tools have been developed and docker, kubernetes and other container-related technologies are very popular among developers.  
Could this technology benefit also embedded devices?  
In the beginning, most of embedded devices were running fully custom made software, as old ships were designed for a specific cargo.  
In the 90s many devices started to run operating systems, providing a reliable foundation for many different kinds of applications.
As intermodal containers did not completely replace dedicated ships, operating systems did not replace dedicated firmware that is still very common on small dedicated devices for the advantages it offers in terms of full-control and resource usage.  
Nowadays more and more of those devices are integrated into IoT solutions where security, reliability, and requirement for new features are pushing the "traditional" ad-hoc embedded development lifecycle to a more agile and dynamic model.  
Software should be developed, updated and distributed quickly and this may not match the development cycle currently followed embedded developers.  
Tightly integrated hardware and software solutions made OS and application inter-dependent and the single-image deployment model, that fits well user-triggered firmware updates, may not work in scenarios where updates are very frequent and critical for the security and reliability of the whole infrastructure.  
Also having devices that need to interoperate with services over the network adds complexity to the application layer and this complexity requires more high-level tools to be handled in a productive way.  
When Linux started to gain popularity on devices, many people complained that running an operating system that was designed for servers (in the Unix era) and popular on personal computers did not make sense. They have been proved wrong by a large number of successful devices designed with Linux as their core OS.  
Today the need to build more complex solution may be driving another server-side technology into the embedded space.  
There are container implementations that can run different operating systems, but in this paper we will focus on Linux-based containers.

## Containers in brief ##

A container is composed by a filesystem and some configuration information that will allow a container runtime to execute the application or services contained inside it.  This is called "image" and each running instance is a container (at least this is the naming convention used by docker and most of the other container-related tools).  
Executables inside the container will run on top of the host kernel, as regular processes, but they will be allowed to access only the contents of the container filesystem or additional folder or files specifically mounted when the container is started.  
This means that we could, for example, run a debian system on a host that runs fedora.  
The container runtime, leveraging some kernel features, will be able to limit CPU and memory usage of the processes running inside a specific container, and the way they communicate over the network can also be defined by the runtime.  
Containers provide two main advantages:

- a standard way to package an application and its dependencies
- isolation between different containers running on the same host

Other solution like standard software packages (deb and rpm, to mention the most popular ones on Linux) or virtualization can provide similar advantages, we will compare advantages and disadvantages of containers compared to those solutions.

## Containers as a standard way to package applications ##

As we mentioned, containers have their own "private" filesystem, this will allow developers to pack all the libraries, tools, configuration files their application needs into a single package. This should solve the infamous "it runs on my machine" scenario where different versions of some OS or shared components on a machine may prevent an application from running correctly. 
Distribution packages also describe all dependencies and try to integrate multiple components using dependencies. Sharing components (for example the C library) is a more efficient way to run applications, it will reduce memory footprint and reduce duplication, but will force all applications using a specific component to use the same version.
Containers will typically use more memory and storage because some files will be duplicated on different containers, but on the other side will grant that a new library required to run an application will not break other apps relying on a different release.
This will also grant that updating a container will have no impact on all the other containers running on the same system and will let different parts of your solution (for example an execution engine, a browser-based remote UI, a local UI) to have different release cycles, not forcing a full update to fix just a single application that requires an updated shared component.
Containers could also be moved between development/testing and production environment easily since they will not require updates of the underlying OS. This would fit well in continuous integration and continuous delivery infrastructure where software is no longer update in monolithic big releases.

## Containers as "lightweight virtual machines" ##

The definition in the title is not one-hundred percent correct.  Actually, containers don't require any additional virtualization support other than the one provided by standard process isolation on Linux, so they are not virtual machines at all.  
On the other side, virtual machines are often used to run different "flavors" of Linux on the same host, providing different services, and this can also be done using containers.  
Virtual machines run a whole OS, kernel and user mode, inside a "sandbox", this will grant more reliability, but requires more resources and adds some overhead because native peripheral devices must be emulated inside the virtual machine.  
Virtual machines are completely separate from the host system and other VMs, this makes them more secure. A potential exploit, even one that allows access to kernel mode, will be limited to the context of a single service and not affect all of them, because they will be running on separated virtual machines. Containers, on the other side, are running regular processes, a user mode exploit will not be able to access the host filesystem but an exploit getting access to kernel mode will probably do damage to the host and other containers.  
Virtualization leverages hardware-specific features while containers can run on processors that don't support virtualization in hardware and, if the runtime allows them to do it, can access peripheral devices using the host kernel, without additional layers in between.  
Containers can provide some degree of separation between different applications and services with less overhead than virtual machines, this justifies the definition.  

## Containers for developers ##

Today software running on devices is getting more and more complex. Devices are becoming part of larger systems and becoming critical for enterprises and organizations. Developing complex software in a reasonable time is challenging and most of the tools that can improve the development experience and help developers during the whole lifecycle of their software are targeting containerized applications.  
Building a CI/CD pipeline often involves containers.  
Many development languages and tools like python, node.js, .NET core are already widely used in a "containerized" version.  
Supporting containers on devices will allow a large number of web/cloud/application developers to be immediately productive in embedded development, as it happened a couple of decades ago when Windows and Linux developers could move to embedded development on Windows CE or embedded Linux.  
Containers will require some additional resources, runtimes will add some performance overhead, but on medium-small projects developed by small teams, this can be compensated with higher productivity and the flexibility provided by a truly agile development lifecycle.  
Containers are here to stay in cloud, server and application development, as Linux was here twenty years ago. They will not fit all scenarios and for some use cases, the additional requirement in terms of resources and infrastructure could not be justified by the advantages, but knowing this technology and evaluate its application on new projects would probably be a worthwhile investment.  
